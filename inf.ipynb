{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import datasets\n",
    "dataset = load_from_disk(\"./resources/data/train_dataset\")\n",
    "val_dataset = dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(val_dataset, datasets.Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': None,\n",
       " 'format_kwargs': {},\n",
       " 'columns': ['title',\n",
       "  'context',\n",
       "  'question',\n",
       "  'id',\n",
       "  'answers',\n",
       "  'document_id',\n",
       "  '__index_level_0__'],\n",
       " 'output_all_columns': False}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': None,\n",
       " 'format_kwargs': {},\n",
       " 'columns': ['title',\n",
       "  'context',\n",
       "  'question',\n",
       "  'id',\n",
       "  'answers',\n",
       "  'document_id',\n",
       "  '__index_level_0__'],\n",
       " 'output_all_columns': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset.set_format(\n",
    "                type=val_dataset.format[\"type\"],\n",
    "                columns=list(val_dataset.features.keys()),\n",
    "            )\n",
    "val_dataset.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '전효숙',\n",
       " 'context': '순천여자고등학교 졸업, 1973년 이화여자대학교를 졸업하고 1975년 제17회 사법시험에 합격하여 판사로 임용되었고 대법원 재판연구관, 수원지법 부장판사, 사법연수원 교수, 특허법원 부장판사 등을 거쳐 능력을 인정받았다. 2003년 최종영 대법원장의 지명으로 헌법재판소 재판관을 역임하였다.\\\\n\\\\n경제민주화위원회(위원장 장하성이 소액주주들을 대표해 한보철강 부실대출에 책임이 있는 이철수 전 제일은행장 등 임원 4명을 상대로 제기한 손해배상청구소송에서 서울지방법원 민사합의17부는 1998년 7월 24일에 \"한보철강에 부실 대출하여 은행에 막대한 손해를 끼친 점이 인정된다\"며 \"원고가 배상을 청구한 400억원 전액을 은행에 배상하라\"고 하면서 부실 경영인에 대한 최초의 배상 판결을 했다. \\\\n\\\\n2004년 10월 신행정수도의건설을위한특별조치법 위헌 확인 소송에서 9인의 재판관 중 유일하게 각하 견해를 내었다. 소수의견에서 전효숙 재판관은 다수견해의 문제점을 지적하면서 관습헌법 법리를 부정하였다. 전효숙 재판관은 서울대학교 근대법학교육 백주년 기념관에서 열린 강연에서, 국회가 고도의 정치적인 사안을 정치로 풀기보다는 헌법재판소에 무조건 맡겨서 해결하려는 자세는 헌법재판소에게 부담스럽다며 소회를 밝힌 바 있다.',\n",
       " 'question': '처음으로 부실 경영인에 대한 보상 선고를 받은 회사는?',\n",
       " 'id': 'mrc-0-003264',\n",
       " 'answers': {'answer_start': [284], 'text': ['한보철강']},\n",
       " 'document_id': 9027,\n",
       " '__index_level_0__': 2146}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "from transformers import HfArgumentParser, AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
    "from peft import get_peft_model, LoraConfig\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2.5-1.5B-Instruct')\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        # bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'Qwen/Qwen2.5-1.5B-Instruct',\n",
    "    from_tf=bool(\".ckpt\" in 'Qwen/Qwen2.5-1.5B-Instruct'),\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "lora_config = LoraConfig(r=8, lora_alpha=16, lora_dropout=0.1, bias=\"none\", task_type=\"CAUSAL_LM\", modules_to_save=None,)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['prompt'],\n",
       "     num_rows: 3952\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt'],\n",
       "     num_rows: 240\n",
       " }))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.data_processing import GenerationDataModule\n",
    "dm = GenerationDataModule(data_args, training_args, tokenizer) \n",
    "train_dataset, eval_dataset = dm.get_processing_data()\n",
    "train_dataset, eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--model_name_or_path MODEL_NAME_OR_PATH]\n",
      "                             [--dataset_name DATASET_NAME]\n",
      "                             [--overwrite_cache [OVERWRITE_CACHE]]\n",
      "                             [--preprocessing_num_workers PREPROCESSING_NUM_WORKERS]\n",
      "                             [--max_answer_length MAX_ANSWER_LENGTH]\n",
      "                             [--output_dir OUTPUT_DIR]\n",
      "                             [--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]\n",
      "                             [--do_train [DO_TRAIN]] [--no_do_train]\n",
      "                             [--do_eval [DO_EVAL]] [--no_do_eval]\n",
      "                             [--do_predict [DO_PREDICT]]\n",
      "                             [--eval_strategy {no,steps,epoch}]\n",
      "                             [--prediction_loss_only [PREDICTION_LOSS_ONLY]]\n",
      "                             [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n",
      "                             [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\n",
      "                             [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n",
      "                             [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
      "                             [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "                             [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]\n",
      "                             [--eval_delay EVAL_DELAY]\n",
      "                             [--torch_empty_cache_steps TORCH_EMPTY_CACHE_STEPS]\n",
      "                             [--learning_rate LEARNING_RATE]\n",
      "                             [--weight_decay WEIGHT_DECAY]\n",
      "                             [--adam_beta1 ADAM_BETA1]\n",
      "                             [--adam_beta2 ADAM_BETA2]\n",
      "                             [--adam_epsilon ADAM_EPSILON]\n",
      "                             [--max_grad_norm MAX_GRAD_NORM]\n",
      "                             [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "                             [--max_steps MAX_STEPS]\n",
      "                             [--lr_scheduler_type LR_SCHEDULER_TYPE]\n",
      "                             [--lr_scheduler_kwargs LR_SCHEDULER_KWARGS]\n",
      "                             [--warmup_ratio WARMUP_RATIO]\n",
      "                             [--warmup_steps WARMUP_STEPS]\n",
      "                             [--log_level {detail,debug,info,warning,error,critical,passive}]\n",
      "                             [--log_level_replica {detail,debug,info,warning,error,critical,passive}]\n",
      "                             [--log_on_each_node [LOG_ON_EACH_NODE]]\n",
      "                             [--no_log_on_each_node]\n",
      "                             [--logging_dir LOGGING_DIR]\n",
      "                             [--logging_strategy {no,steps,epoch}]\n",
      "                             [--logging_first_step [LOGGING_FIRST_STEP]]\n",
      "                             [--logging_steps LOGGING_STEPS]\n",
      "                             [--logging_nan_inf_filter [LOGGING_NAN_INF_FILTER]]\n",
      "                             [--no_logging_nan_inf_filter]\n",
      "                             [--save_strategy {no,steps,epoch}]\n",
      "                             [--save_steps SAVE_STEPS]\n",
      "                             [--save_total_limit SAVE_TOTAL_LIMIT]\n",
      "                             [--save_safetensors [SAVE_SAFETENSORS]]\n",
      "                             [--no_save_safetensors]\n",
      "                             [--save_on_each_node [SAVE_ON_EACH_NODE]]\n",
      "                             [--save_only_model [SAVE_ONLY_MODEL]]\n",
      "                             [--restore_callback_states_from_checkpoint [RESTORE_CALLBACK_STATES_FROM_CHECKPOINT]]\n",
      "                             [--no_cuda [NO_CUDA]] [--use_cpu [USE_CPU]]\n",
      "                             [--use_mps_device [USE_MPS_DEVICE]] [--seed SEED]\n",
      "                             [--data_seed DATA_SEED]\n",
      "                             [--jit_mode_eval [JIT_MODE_EVAL]]\n",
      "                             [--use_ipex [USE_IPEX]] [--bf16 [BF16]]\n",
      "                             [--fp16 [FP16]] [--fp16_opt_level FP16_OPT_LEVEL]\n",
      "                             [--half_precision_backend {auto,apex,cpu_amp}]\n",
      "                             [--bf16_full_eval [BF16_FULL_EVAL]]\n",
      "                             [--fp16_full_eval [FP16_FULL_EVAL]] [--tf32 TF32]\n",
      "                             [--local_rank LOCAL_RANK]\n",
      "                             [--ddp_backend {nccl,gloo,mpi,ccl,hccl,cncl,mccl}]\n",
      "                             [--tpu_num_cores TPU_NUM_CORES]\n",
      "                             [--tpu_metrics_debug [TPU_METRICS_DEBUG]]\n",
      "                             [--debug DEBUG [DEBUG ...]]\n",
      "                             [--dataloader_drop_last [DATALOADER_DROP_LAST]]\n",
      "                             [--eval_steps EVAL_STEPS]\n",
      "                             [--dataloader_num_workers DATALOADER_NUM_WORKERS]\n",
      "                             [--dataloader_prefetch_factor DATALOADER_PREFETCH_FACTOR]\n",
      "                             [--past_index PAST_INDEX] [--run_name RUN_NAME]\n",
      "                             [--disable_tqdm DISABLE_TQDM]\n",
      "                             [--remove_unused_columns [REMOVE_UNUSED_COLUMNS]]\n",
      "                             [--no_remove_unused_columns]\n",
      "                             [--label_names LABEL_NAMES [LABEL_NAMES ...]]\n",
      "                             [--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]\n",
      "                             [--no_load_best_model_at_end]\n",
      "                             [--metric_for_best_model METRIC_FOR_BEST_MODEL]\n",
      "                             [--greater_is_better [GREATER_IS_BETTER]]\n",
      "                             [--no_greater_is_better]\n",
      "                             [--ignore_data_skip [IGNORE_DATA_SKIP]]\n",
      "                             [--fsdp FSDP]\n",
      "                             [--fsdp_min_num_params FSDP_MIN_NUM_PARAMS]\n",
      "                             [--fsdp_config FSDP_CONFIG]\n",
      "                             [--fsdp_transformer_layer_cls_to_wrap FSDP_TRANSFORMER_LAYER_CLS_TO_WRAP]\n",
      "                             [--accelerator_config ACCELERATOR_CONFIG]\n",
      "                             [--deepspeed DEEPSPEED]\n",
      "                             [--label_smoothing_factor LABEL_SMOOTHING_FACTOR]\n",
      "                             [--optim OPTIM] [--optim_args OPTIM_ARGS]\n",
      "                             [--adafactor [ADAFACTOR]]\n",
      "                             [--group_by_length [GROUP_BY_LENGTH]]\n",
      "                             [--length_column_name LENGTH_COLUMN_NAME]\n",
      "                             [--report_to REPORT_TO]\n",
      "                             [--ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS]\n",
      "                             [--ddp_bucket_cap_mb DDP_BUCKET_CAP_MB]\n",
      "                             [--ddp_broadcast_buffers DDP_BROADCAST_BUFFERS]\n",
      "                             [--dataloader_pin_memory [DATALOADER_PIN_MEMORY]]\n",
      "                             [--no_dataloader_pin_memory]\n",
      "                             [--dataloader_persistent_workers [DATALOADER_PERSISTENT_WORKERS]]\n",
      "                             [--skip_memory_metrics [SKIP_MEMORY_METRICS]]\n",
      "                             [--no_skip_memory_metrics]\n",
      "                             [--use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP]]\n",
      "                             [--push_to_hub [PUSH_TO_HUB]]\n",
      "                             [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
      "                             [--hub_model_id HUB_MODEL_ID]\n",
      "                             [--hub_strategy {end,every_save,checkpoint,all_checkpoints}]\n",
      "                             [--hub_token HUB_TOKEN]\n",
      "                             [--hub_private_repo [HUB_PRIVATE_REPO]]\n",
      "                             [--hub_always_push [HUB_ALWAYS_PUSH]]\n",
      "                             [--gradient_checkpointing [GRADIENT_CHECKPOINTING]]\n",
      "                             [--gradient_checkpointing_kwargs GRADIENT_CHECKPOINTING_KWARGS]\n",
      "                             [--include_inputs_for_metrics [INCLUDE_INPUTS_FOR_METRICS]]\n",
      "                             [--eval_do_concat_batches [EVAL_DO_CONCAT_BATCHES]]\n",
      "                             [--no_eval_do_concat_batches]\n",
      "                             [--fp16_backend {auto,apex,cpu_amp}]\n",
      "                             [--evaluation_strategy EVALUATION_STRATEGY]\n",
      "                             [--push_to_hub_model_id PUSH_TO_HUB_MODEL_ID]\n",
      "                             [--push_to_hub_organization PUSH_TO_HUB_ORGANIZATION]\n",
      "                             [--push_to_hub_token PUSH_TO_HUB_TOKEN]\n",
      "                             [--mp_parameters MP_PARAMETERS]\n",
      "                             [--auto_find_batch_size [AUTO_FIND_BATCH_SIZE]]\n",
      "                             [--full_determinism [FULL_DETERMINISM]]\n",
      "                             [--torchdynamo TORCHDYNAMO]\n",
      "                             [--ray_scope RAY_SCOPE]\n",
      "                             [--ddp_timeout DDP_TIMEOUT]\n",
      "                             [--torch_compile [TORCH_COMPILE]]\n",
      "                             [--torch_compile_backend TORCH_COMPILE_BACKEND]\n",
      "                             [--torch_compile_mode TORCH_COMPILE_MODE]\n",
      "                             [--dispatch_batches DISPATCH_BATCHES]\n",
      "                             [--split_batches SPLIT_BATCHES]\n",
      "                             [--include_tokens_per_second [INCLUDE_TOKENS_PER_SECOND]]\n",
      "                             [--include_num_input_tokens_seen [INCLUDE_NUM_INPUT_TOKENS_SEEN]]\n",
      "                             [--neftune_noise_alpha NEFTUNE_NOISE_ALPHA]\n",
      "                             [--optim_target_modules OPTIM_TARGET_MODULES]\n",
      "                             [--batch_eval_metrics [BATCH_EVAL_METRICS]]\n",
      "                             [--eval_on_start [EVAL_ON_START]]\n",
      "                             [--use_liger_kernel [USE_LIGER_KERNEL]]\n",
      "                             [--eval_use_gather_object [EVAL_USE_GATHER_OBJECT]]\n",
      "                             [--dataset_text_field DATASET_TEXT_FIELD]\n",
      "                             [--packing [PACKING]]\n",
      "                             [--max_seq_length MAX_SEQ_LENGTH]\n",
      "                             [--dataset_num_proc DATASET_NUM_PROC]\n",
      "                             [--dataset_batch_size DATASET_BATCH_SIZE]\n",
      "                             [--model_init_kwargs MODEL_INIT_KWARGS]\n",
      "                             [--dataset_kwargs DATASET_KWARGS]\n",
      "                             [--eval_packing EVAL_PACKING]\n",
      "                             [--num_of_sequences NUM_OF_SEQUENCES]\n",
      "                             [--chars_per_token CHARS_PER_TOKEN]\n",
      "                             [--use_liger [USE_LIGER]]\n",
      "ipykernel_launcher.py: error: ambiguous option: --f=/data/ephemeral/home/.local/share/jupyter/runtime/kernel-v334bc2574705adcd862f2f3674f7f7b1b183e6b41.json could match --fp16, --fp16_opt_level, --fp16_full_eval, --fsdp, --fsdp_min_num_params, --fsdp_config, --fsdp_transformer_layer_cls_to_wrap, --fp16_backend, --full_determinism\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/level2-mrc-nlp-08/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from model.qat_custom import GenerationTrainer\n",
    "from utils.arguments_gen_reader import ModelArguments, OurTrainingArguments, DataTrainingArguments\n",
    "from utils.data_processing import GenerationDataModule\n",
    "from evaluate import load\n",
    "\n",
    "parser = HfArgumentParser(\n",
    "        (ModelArguments, DataTrainingArguments, OurTrainingArguments) # arguement 쭉 읽어보면서 이해하기\n",
    "    )\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "dm = GenerationDataModule(data_args, training_args, tokenizer) \n",
    "train_dataset, eval_dataset = dm.get_processing_data()\n",
    "metric = load('squad')\n",
    "trainer = GenerationTrainer(\n",
    "        model=model,\n",
    "        dataset_text_field=\"prompt\",\n",
    "        max_seq_length=2000,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        peft_config=lora_config,\n",
    "        tokenizer=tokenizer,\n",
    "        packing= training_args.packing,\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "        metrics=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef469a92eee446a383849cd24abe1175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "import torch\n",
    "\n",
    "# 기본 모델 경로 (원본 모델의 경로여야 합니다)\n",
    "base_model_path = \"beomi/Qwen2.5-7B-Instruct-kowiki-qa-context\"\n",
    "\n",
    "# Checkpoint 경로\n",
    "checkpoint_path = \"./resources/checkpoint/generation/checkpoint-1482\"\n",
    "\n",
    "# PEFT 설정 로드\n",
    "peft_config = PeftConfig.from_pretrained(checkpoint_path)\n",
    "\n",
    "# 기본 모델 경로\n",
    "base_model_path = peft_config.base_model_name_or_path\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)\n",
    "\n",
    "# 양자화 설정\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "# 기본 모델 로드\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "# PEFT 모델 로드\n",
    "model = PeftModel.from_pretrained(base_model, checkpoint_path)\n",
    "\n",
    "# 모델 평가 모드로 설정\n",
    "\n",
    "def generate_prompt(question, context):\n",
    "    # Qwen prefix\n",
    "    prefix_chat_template = '''<|im_start|>system\n",
    "    You are Qwen, created by Alibaba Cloud. You are a helpful assistant. 모든 대답은 한국어로 해주세요.<|im_end|>\n",
    "    <|im_start|>user\n",
    "    question:{} \n",
    "    context:{}<|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    <|im_end|>'''\n",
    "\n",
    "    # prefix에 formatting\n",
    "    prompts = [prefix_chat_template.format(question, context)]\n",
    "    # 데이터에 prompt 추가\n",
    "    return prompts\n",
    "\n",
    "# 추론 함수\n",
    "def inference(question, context):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(generate_prompt(question, context), return_tensors=\"pt\", padding=True, truncation=True, max_length=2000)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    print(inputs)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[151644,   8948,    198,    262,   1446,    525,   1207,  16948,     11,\n",
      "           3465,    553,  54364,  14817,     13,   1446,    525,    264,  10950,\n",
      "          17847,     13, 129304,  60960, 132760,  33704, 130092,  31079,  17380,\n",
      "          60716,  91669,     13, 151645,    198,    257, 151644,    872,    198,\n",
      "            262,   3405,   4884, 144410, 144410,      6,  33704, 134664, 128552,\n",
      "          85413,    225,  48431,  43590, 133886, 139053,  47836,  53618,  40720,\n",
      "         132306,     13, 125206,  82528,  49543, 129889,  85413,    225,  16560,\n",
      "         136665,  17877, 137352, 128552,  63332,  57026, 131303,  28733,  46682,\n",
      "          20487, 129147, 124970,  32831,  31079,  18411,  40720,  42905,  49052,\n",
      "          19969, 129875, 124419,  54825,  70943,  56475,    364, 145329, 145329,\n",
      "            516,    364, 147758, 147758,      6,  17877,  83315,  32831,  49543,\n",
      "          17380,  81718, 136892,  31079,  40720,  23573, 128584,    364, 144410,\n",
      "         144410,      6,  19391,  94613,  51876,     13,  23084,  19391, 131978,\n",
      "            220,     17,     15,     15,     16, 126216, 131698, 129889,    330,\n",
      "          25715,  48431,  72553, 131670, 139052,  29326,  82190, 125206,  82528,\n",
      "          55902,  20401, 129242,  56039,  18411, 126366,  66425, 130679, 130864,\n",
      "          42039,  40720, 130357,  90686,      1,  34395,  60716, 129150,  23573,\n",
      "          81718,  90686,     13,   4710, 144410, 135184,  85413,    225,  48431,\n",
      "          43590, 133886, 132989,  42905, 132091,  40720,  47836,  53618,     11,\n",
      "            364, 144410,      6,  25715,  61298,  73523,  56475,    364, 144410,\n",
      "         144410, 144410, 144410, 144410, 144410, 144410, 144410, 144410, 144410,\n",
      "          73338,  42905,  28927,    251,  42039, 130847,  74361,    222, 130408,\n",
      "          61298,  53989,    226, 143835,  57801, 128878,   3315,    241,    108,\n",
      "          12802, 130898, 129112,    220, 144410, 135184, 138667, 130966,   3315,\n",
      "            235,    120, 139036,  19391, 126629,  54825, 143863,  53680, 132989,\n",
      "          16560,  34143,    105,  50340,  85251,  13146,     13, 128753,  92817,\n",
      "          31079,  18411,  56419,  51588,  42039, 131698,  42905,  23084,  57132,\n",
      "         126407,  53435,  56290, 126742, 126605, 132184,    220,  26940,  23573,\n",
      "         124785,  32077,  41671,  25067,  20401,  54116,  34395,  83291,  56475,\n",
      "          54825,  98358,  97929,  80573, 143863,  12802,  60960, 138279, 139107,\n",
      "          78374, 131042,  63332, 137750,    198,      9,    220, 144410,    481,\n",
      "         134664, 128552, 125149, 125512, 128555, 126322,  20401,  16186, 125476,\n",
      "          16560,  58034, 130803,  56475,   3315,    241,    108,  31328,  13146,\n",
      "             13, 128772, 126254, 134539,  19391,  62740,    247, 124982,  17877,\n",
      "          53618,     11, 136342, 130229,  32290,    364, 139772, 144410,    516,\n",
      "            364, 144388, 126730, 144410,    516,    364, 130137, 144410,      6,\n",
      "          77002,  33704,  85413,    225,  48431, 136751,  50340,  20487, 129885,\n",
      "          16560,  38150, 144326,  28002,  19969, 138652, 127166, 144183,  38523,\n",
      "            105,  50340,  62275,  58034, 130803, 131411, 143863,  17877, 138267,\n",
      "          13146,     13, 124970,  32831,  31079,  17380,  16560,    330, 147758,\n",
      "              1, 137602,    330, 145329,      1,  42039,  60716, 129150, 131396,\n",
      "          28733,  90686,     13,  61298, 129027,    364, 136593,  33883,     30,\n",
      "         144410,    516,    364,  56039, 126246, 144410,      6,  20401,  49052,\n",
      "            364, 144410,      6,  16560,  58034, 129924, 128533, 124459,    108,\n",
      "         127452,  20401, 127864,  47836,  17877, 129112,    624,      9,    220,\n",
      "         144410, 144410,    481,  45710,  20401,  71108, 129885,    220, 144410,\n",
      "         130427, 125703,  19969, 126366, 126735,  33509,  60315, 142212, 141965,\n",
      "          76337,  80968, 131411, 143863,  17877, 138267,  13146,     13, 125953,\n",
      "          57801,  16560,    364,  48606, 124934, 129534,      6,  17877,   5140,\n",
      "            250,    119, 135405,     11, 134422,  73518, 144161,  57801,  16560,\n",
      "          60960,  56290,  55902,  56475, 124970, 131000, 128552,  57835,  93701,\n",
      "         131097,  18411,  65553,     96,  34395, 135511, 143863,  17877, 138267,\n",
      "          13146,     13,    364, 144410, 144410,      6,  33704, 125714, 132557,\n",
      "         131793, 132751, 126253, 130887,  28927,    104,  31079,  42905, 128677,\n",
      "          20401, 130768,  51391,  82528, 126781, 126254, 129924,  17380, 129296,\n",
      "          29281,  64119, 130898,  10764,    244,  19946,     13, 139293,  98005,\n",
      "          55054,  58034,  55054, 126327,  60960, 132760,  42905,  49052, 129381,\n",
      "         125466, 128533,  58034, 130803, 129889,    364, 126730, 144410, 144410,\n",
      "              6,  80573, 129381,  60960, 132760,  33704,  35509, 144509, 130109,\n",
      "          63332,  31328, 130822,  65510,  55054,  47985, 137046,     13,    715,\n",
      "              9,    220, 144410, 144410, 144410,    481, 142350,  80968,  70943,\n",
      "         126702, 133864, 131611,  47985, 130887, 130966,  40720, 128841,  65722,\n",
      "            102, 126270, 125489,     13,  85413,    225,  16560, 129423,  29281,\n",
      "          17877,  54825, 129923, 139053, 136305,   5140,    231,    246, 128205,\n",
      "          24897,  18411, 143143,  16560,  13146,    624,      9,    220, 144410,\n",
      "         144410, 144410, 144410,   1112,    481, 131180,  26698, 126558,  16560,\n",
      "            330,  29281, 127452,  85413,    225, 133507,  13146,      1, 129615,\n",
      "          63757, 131518,  17877,  73518,  63154,  36055,  32831,  64850,  57026,\n",
      "          26698, 139053,  42905, 143863,  17877, 138267,  13146,     13,  65553,\n",
      "             97,  73523, 126558,  54825, 130408,  33704, 132989,  20401,  50972,\n",
      "         126970,  53680,  36055,  26698,  20401, 129413,  47985,  19969, 138313,\n",
      "          78374, 127378,     11, 129007,  18585,    238,  56475,    364, 144410,\n",
      "         144410, 144410, 144410,      6,  16560, 126423, 128732, 128552,  85413,\n",
      "            225, 133507, 130822, 129337, 139053,  42905,  81173,  43590,  65510,\n",
      "         124873,  19391,  94613,  51876,     13,    715,    262,   2266,     25,\n",
      "         138873,  16560, 129423,  29281,  17877,  54825, 129923, 139053, 136305,\n",
      "           5140,    231,    246, 128205,  24897,  18411, 143143,  16560,    220,\n",
      "         144410,  20401,  65722,    102, 126270,  33704,     30, 151645,    198,\n",
      "            257, 151644,  77091,    198,    257, 151645]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "Generated text: system\n",
      "    You are Qwen, created by Alibaba Cloud. You are a helpful assistant. 모든 대답은 한국어로 해주세요.\n",
      "    user\n",
      "    question:'ㅋㅋ'은 일반적으로 웃음소리를 표현할 때 사용된다. 통신체에서는 웃는 모습을 직접적으로 보여줄 수 없기 때문에 의성어를 사용하는 경우가 많은데 그 중에서 '킥킥', '큭큭'을 초성체로 바꾸어 사용한 것이 'ㅋㅋ'에 해당한다. 이에 대해 2001년 연구에서는 \"자음만 가지고 표시하여 통신상의 재미를 더하기 위한 방법으로 사용되고 있다\"고 해석한 바 있다. \n",
      "\n",
      "ㅋ자를 웃음소리를 의미하는 것으로 사용할 때, 'ㅋ'자 한 개에서 'ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ...'하는 식으로 열 타 이상 한 줄 넘게까지 쓰이기도 한다 ㅋ자를 얼마나 많이 썼느냐에 따라 그 느낌과 의미는 달라진다. 신조어를 전문으로 연구하는 이재현 문화평론가는 《한국일보》의 기고글에서 그 종류와 느낌이 대략 다음과 같다고 보았다\n",
      "* ㅋ - 일반적으로 무심하게 동의하려는 상황에서 쓰인다. 다른 말끝에 붙였을 때, 이를테면 '그래ㅋ', '좋네ㅋ', '올ㅋ' 등은 웃음 자체라기보다는 입꼬리가 아주 살짝 올라간 상황이라는 느낌을 준다. 의성어로는 \"큭\" 혹은 \"킥\"으로 해석될 수 있다. 한편 '뭐해?ㅋ', '미안ㅋ'의 경우 'ㅋ'는 상투적인 군말의 역할을 한다.\n",
      "* ㅋㅋ - 위의 것보다 ㅋ자가 하나가 더 많으나 오히려 형식적이라는 느낌을 준다. 좋게는 '그렇군'을 뜻하거나, 조금 나쁘게는 대화상에서 의례적으로 추임새를 넣고 있다는 느낌을 준다. 'ㅋㅋ'은 미혼남녀들이 가장 싫어하는 성의 없는 메신저 말투로 선정되기도 했다. 다만 회사 상사에게 대답하는 경우 같은 공적인 상황에서는 '네ㅋㅋ'와 같은 대답은 가벼워 보인다는 조사도 있었다. \n",
      "* ㅋㅋㅋ - 비교적 중립적이면서도 가장 많이 사용되는 용법이다. 웃는 감정을 그대로 표현한다는 뉘앙스를 갖는다.\n",
      "* ㅋㅋㅋㅋ... - 여기서부터는 \"정말 웃긴다\"라는 반응을 나름 정성들여서 표현하는 느낌을 준다. 네 개부터 그 이상은 의미의 함축과 정서의 강도가 거의 같으며, 이런 점에서 'ㅋㅋㅋㅋ'는 실질적으로 웃긴다는 것을 표현하는 최소 조건에 해당한다. \n",
      "    context:웃는 감정을 그대로 표현한다는 뉘앙스를 갖는 ㅋ의 용법은?\n",
      "    assistant\n",
      "    !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "context = ''''ㅋㅋ'은 일반적으로 웃음소리를 표현할 때 사용된다. 통신체에서는 웃는 모습을 직접적으로 보여줄 수 없기 때문에 의성어를 사용하는 경우가 많은데 그 중에서 '킥킥', '큭큭'을 초성체로 바꾸어 사용한 것이 'ㅋㅋ'에 해당한다. 이에 대해 2001년 연구에서는 \"자음만 가지고 표시하여 통신상의 재미를 더하기 위한 방법으로 사용되고 있다\"고 해석한 바 있다. \\n\\nㅋ자를 웃음소리를 의미하는 것으로 사용할 때, 'ㅋ'자 한 개에서 'ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ...'하는 식으로 열 타 이상 한 줄 넘게까지 쓰이기도 한다 ㅋ자를 얼마나 많이 썼느냐에 따라 그 느낌과 의미는 달라진다. 신조어를 전문으로 연구하는 이재현 문화평론가는 《한국일보》의 기고글에서 그 종류와 느낌이 대략 다음과 같다고 보았다\\n* ㅋ - 일반적으로 무심하게 동의하려는 상황에서 쓰인다. 다른 말끝에 붙였을 때, 이를테면 '그래ㅋ', '좋네ㅋ', '올ㅋ' 등은 웃음 자체라기보다는 입꼬리가 아주 살짝 올라간 상황이라는 느낌을 준다. 의성어로는 \"큭\" 혹은 \"킥\"으로 해석될 수 있다. 한편 '뭐해?ㅋ', '미안ㅋ'의 경우 'ㅋ'는 상투적인 군말의 역할을 한다.\\n* ㅋㅋ - 위의 것보다 ㅋ자가 하나가 더 많으나 오히려 형식적이라는 느낌을 준다. 좋게는 '그렇군'을 뜻하거나, 조금 나쁘게는 대화상에서 의례적으로 추임새를 넣고 있다는 느낌을 준다. 'ㅋㅋ'은 미혼남녀들이 가장 싫어하는 성의 없는 메신저 말투로 선정되기도 했다. 다만 회사 상사에게 대답하는 경우 같은 공적인 상황에서는 '네ㅋㅋ'와 같은 대답은 가벼워 보인다는 조사도 있었다. \\n* ㅋㅋㅋ - 비교적 중립적이면서도 가장 많이 사용되는 용법이다. 웃는 감정을 그대로 표현한다는 뉘앙스를 갖는다.\\n* ㅋㅋㅋㅋ... - 여기서부터는 \"정말 웃긴다\"라는 반응을 나름 정성들여서 표현하는 느낌을 준다. 네 개부터 그 이상은 의미의 함축과 정서의 강도가 거의 같으며, 이런 점에서 'ㅋㅋㅋㅋ'는 실질적으로 웃긴다는 것을 표현하는 최소 조건에 해당한다.'''\n",
    "question = '웃는 감정을 그대로 표현한다는 뉘앙스를 갖는 ㅋ의 용법은?'\n",
    "generated_text = inference(context, question)\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
