{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/sh/level2-mrc-nlp-08/.venv_sh/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:08<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[|system|]You are EXAONE model from LG AI Research, a helpful assistant.[|endofturn|]\n",
      "[|user|]너의 소원을 말해봐\n",
      "[|assistant|]EXAONE 3.0 모델로서, 저의 주된 목적은 사용자에게 정확하고 유용한 정보를 제공하는 것입니다. 저는 다양한 질문에 답변하고, 문제를 해결하며, 학습과 연구를 돕기 위해 설계되었습니다. 제가 도울 수 있는 다른 방법이 있다면 언제든지 말씀해 주세요![|endofturn|]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\")\n",
    "\n",
    "# Choose your prompt\n",
    "prompt = \"Explain who you are\"  # English example\n",
    "prompt = \"너의 소원을 말해봐\"   # Korean example\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \n",
    "     \"content\": \"You are EXAONE model from LG AI Research, a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "print(input_ids)\n",
    "output = model.generate(\n",
    "    input_ids.to(\"cuda\"),\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=128\n",
    ")\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/sh/level2-mrc-nlp-08/.venv_sh/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk(\"./resources/data/train_dataset/\")\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '타지마할',\n",
       " 'context': \"타지마할의 정원은 한 변의 길이가 약 300m에 이르는 페르시아 양식과 힌두 양식이 뒤섞인 전통적 무굴 양식의 정원이다. 정원에는 축을 따라 석조 보도가 깔려 있는데, 이 석조 보도들이 격자형으로 뻗어나가 정원을 모두 16개의 화단으로 나눈다. 타지마할의 정문과 영묘 본 건물 사이에는 남북축을 따라 건설한, 대리석으로 만든 수로가 존재하는데, 이 수로는 영묘의 모습을 고스란히 반사하여 정문으로 입장하는 사람들에게 인상적인 시각적 효과를 선사하도록 설계되어 있다. 이 수로의 이름은 '알 하우드 알 카후타'이며, '풍요의 수로'라는 뜻을 갖고 있으며, 이슬람의 선지자 무함마드에게 봉헌된 것이다. \\\\n\\\\n또한 정원 주위에는 여러 그루의 나무들이 줄지어 심겨있고, 그 외에 물을 뿜어내는 분수들이 존재한다. 무굴 양식의 정원은 무굴 제국의 초대 황제인 바부르에 의해 만들어진 것인데, 전통적 인도 양식의 정원과 페르시아 양식의 정원이 합쳐진 모습을 띠고 있다. 무굴 정원은 이슬람교의 천국을 묘사하였기 때문에 총 4개의 강, 혹은 수로들이 그 안에 흐르고 있고, 그 밖에 높은 벽들이 둘러치고 있는 형태이다. 무굴 제국 시대에 쓰여진 기록에 의하면, 대부분 천당은 높은 산 위에서 흘러내린 4개의 강들이 흐르고 있고, 그 주변 동서남북을 높은 벽들로 감싼 형태로 묘사되기 때문이다. \\\\n\\\\n거의 대부분의 무굴 정원들은 그 중앙에 중요한 건물, 혹은 영묘가 있지만, 타지마할의 경우에는 정원 맨 끝에 영묘가 존재한다는 점에서 나름 독창적인 형태를 하고 있다고 여겨졌다. 하지만 현대에 들어, 야무나 강 반대쪽에 '달빛 정원'(Moonlight Garden)이 발견됨에 따라, 야무나 강 자체가 타지마할 정원의 일부였다는 점이 밝혀짐에 따라, 타지마할 또한 여타 정원들과 같이 정원의 한가운데에 영묘가 위치하는 구조로 지어졌다는 것이 입증되게 되었다. 인도의 고고학계는 타지마할의 설계자들이 야무나 강을 천국의 4개 강들 중 하나를 상징하는 것으로 표현하기 위해 이와 같은 구조를 취했다고 추정하고 있다. 타지마할에 대한 초기 기록은 이 정원에서 풍부한 과육들이 자랐다고 적고 있는데, 주로 장미, 수선화, 과일 나무 등이 자랐다고 한다. 하지만 시간이 흘러 무굴 제국이 쇠퇴함에 따라, 타지마할의 관리도 점차 소홀해졌다. 인도 아대륙의 5분의 3의 지배자로 떠오른 대영제국이 이후 타지마할의 관리를 맡게 되었고, 이들은 자신들의 취향에 맞춰 타지마할의 정원을 런던에 있는 전형적인 영국식 정원으로 개조하려 시도하였다.\",\n",
       " 'question': '타지마할의 실제 구조를 밝히는데 도움을 준 것은?',\n",
       " 'id': 'mrc-0-003146',\n",
       " 'answers': {'answer_start': [789], 'text': [\"'달빛 정원'(Moonlight Garden)\"]},\n",
       " 'document_id': 16093,\n",
       " '__index_level_0__': 2069}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  420,   453, 47982,   453,   422,  5094,   937, 11522,   394,  5746,\n",
       "          1932,  1005,  7401, 10680,  8385,   373,   619, 12913, 19415,   375,\n",
       "           361,   560,   420,   453, 14719,   453,   422,  2088,   730, 19658,\n",
       "           696,  1216,   999,  7000,   560,   420,   453,  1167,  8659,   453,\n",
       "           422]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:08<00:00,  1.23s/it]\n"
     ]
    }
   ],
   "source": [
    "# pretrained model 과 tokenizer를 불러오기\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./resources/checkpoint/generation/checkpoint-1000\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./resources/checkpoint/generation/checkpoint-1000\", load_in_4bit=True, device_map=\"auto\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred : [|system|] You are EXAONE model from LG AI Research, a helpful assistant. [|endofturn|]\n",
      "        [|user|] Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that \"없음\", don't try to make up an answer. Please answer in short answer. Keep the answer as concise.\n",
      "        초기인 1985년의 토큰링 네트워크는 전송속도가 4Mbps였지만, 1989년에 IBM이 16Mbps의 토큰링 제품을 생산하였으며, IEEE 802.5 기준도 이를 지원하도록 확장되었다.\\n\\n토큰링 네트워크는 여러 스테이션(컴퓨터)들이 하나의 링(고리)에 이어져 형성되며, 데이터는 항상 한 방향으로만 흐른다. 정확히 말하면 데이터가 한 컴퓨터에서 다음 컴퓨터로 순서대로 전달된다. 각각의 스테이션은 바로 이전의 스테이션이 전달해준 비트를 그대로 다음 스테이션에 전달해주는 역할을 수행한다.\\n\\n보통 3바이트로 이루어진 하나의 제어토큰이 생성되어 한 방향으로 링을 순환하며 스테이션들의 네트워크 접속을 제어한다. 만약 어느 스테이션도 전송을 하고 있지 않다면, 이 제어토큰 프레임이 링을 끊임없이 순환한다.\\n\\n데이터 프레임을 같은 네트워크 내의 스테이션에 전달하고자 하는 스테이션은 이 토큰을 획득하여야만 전송을 할 수 있다. 정확하게 말하면, 이 스테이션은 토큰이 돌아오기를 기다렸다가, 토큰이 전달되면 그것을 다음 컴퓨터에게 전달하지않고, 전송하고자 하는 프레임을 내보낸다. 그 다음의 스테이션들은 이 데이터를 그대로 다음 스테이션으로 넘기고, 이 프레임의 수신자에게 도착하였을때, 이 수신자는 이 프레임의 복사본을 만들어 보관한다.\\n\\n하지만 이 스테이션 또한 계속 그 프레임을 다음 스테이션에 전송한다. 그러다가 최초의 전송자에게 이 프레임이 돌아왔을때, 전송자는 이 프레임을 흡수하고 원래의 토큰 프레임을 다음 컴퓨터에게 전송한다.\\n\\n토큰링은 처음엔 이더넷보다 이론적으로 빠르고 안정적인 기술로 각광받았으나, 후에 스위치 이더넷이 개발되면서 급격하게 쇠퇴하고 말았다.\n",
      "        Question:토큰링이 빠른 속도로 퇴보하는데 영향을 준 기술은?\n",
      " IP 기반의 이더넷이었다.[|endofturn|]\n",
      "answer : ['스위치 이더넷']\n",
      "---------------------------------------------------------------------\n",
      "pred : [|system|] You are EXAONE model from LG AI Research, a helpful assistant. [|endofturn|]\n",
      "        [|user|] Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that \"없음\", don't try to make up an answer. Please answer in short answer. Keep the answer as concise.\n",
      "        향로란 절에서 마음의 때를 씻는다는 의미의 향을 피우는데 사용하는 도구이다.\\n\\n이 향로는 높이 40.1cm, 입 지름 30cm의 크기로 받침, 몸체, 입 3부분으로 되어있다. 입은 수평으로 넓게 퍼진 테를 가진 전이 있으며, 그 전을 구슬 무늬로 장식하고 연꽃과 덩굴 무늬를 새기고 은을 입혔다. 몸체에는 대칭되는 위치에 꽃으로 창을 만들고 그 안에 용과 봉황을 세밀하게 은입사하였다. 남은 공간에는 갈대와 연꽃을 새기고 위에는 기러기를 새기고, 밑에 오리를 새겨 은을 입혔다.\\n\\n나팔형 받침은 위의 가장자리를 쌍선으로 굵게 표시하고, 위로 오르면서 덩굴무늬를, 하단에는 풀무늬를, 굽에는 꽃무늬를 은입사하였다. 은입사 문양은 모두 뛰어난 솜씨를 보여줄 뿐아니라 회화적 가치도 아주 높다. 받침 굽에는 34자의 글씨가 남아 있어, 이 향로가 충렬왕 15년(1289)에 제작되어 개풍군 흥왕사에 있었던 것임을 알려준다.\\n\\n세련된 은입사 기법과 연대를 알 수 있다는 점과 잘 사용하지 않던 용과 봉황을 문양으로 사용한 점은 이 향로의 가치를 높여 준다.\n",
      "        Question:받침 굽에 새겨져 있는 글자의 수는?\n",
      " IP:192.168.1.100:8080\n",
      "Program STARD\n",
      "answer : ['34자']\n",
      "---------------------------------------------------------------------\n",
      "pred : [|system|] You are EXAONE model from LG AI Research, a helpful assistant. [|endofturn|]\n",
      "        [|user|] Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that \"없음\", don't try to make up an answer. Please answer in short answer. Keep the answer as concise.\n",
      "        오야코돈\\n오야코동(親子丼)은 닭고기를 조미된 국물로 끓여낸 뒤, 달걀물을 풀어 익힌 후 밥 위에 얹은 음식이다. 먹는 위치에 따라 양파나 대파 등을 곁들이는 곳도 있다. 오야코(부모자식)라는 말은 얹어지는 재료가 닭고기와 달걀이라는 점에서 유래했다. 하지만 오야코라는 말 자체가 닭과 달걀을 의미하는 단어가 아니기 때문에, 일반적이지는 않지만 다른 식재료를 써서 만드는 경우도 있다. 이런 변형된 오야코동은 오리고기와 오리알, 연어와 연어알, 청어와 청어알 등 다양한 식재가 사용될 수 있다.\\n 우나돈=== 우나동 ===\\n 우나동(鰻丼)은 장어를 가바야키로 조리한 뒤 밥에 얹어 먹는 요리이며, 에도의 향토 요리이기도 하다. 양념장은 주로 간장과 설탕을 가지고 만들어지며, 이 소스는 장어를 구울때도 바르지만 일반적으로 다 구워진 장어를 밥 위에 얹은 뒤 다시 발라낸다. 산초 가루를 마지막에 뿌리는데, 이는 기름진 장어를 산뜻하게 먹고 소화를 돕기 위함이다. 우나주(鰻重)라는 장어 요리 역시 우나동과 조리 방법이 거의 동일한데, 위에 설명된 요리를 찬합에 담으면 우나주가 되며, 돈부리바치에 담으면 우나동이라 부른다.\n",
      "        Question:우나동의 주요 식재료는?\n",
      " IP:192.168.1.100\n",
      "|오야코돈|우나\n",
      "answer : ['장어']\n",
      "---------------------------------------------------------------------\n",
      "pred : [|system|] You are EXAONE model from LG AI Research, a helpful assistant. [|endofturn|]\n",
      "        [|user|] Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that \"없음\", don't try to make up an answer. Please answer in short answer. Keep the answer as concise.\n",
      "        아카데미상은 1945년부터 미국 영화가 아닌 외국어 영화를 대상으로도 시상하고 있다. 아카데미 측은 외국어영화 부문 후보 작품을 두고 '미국 국외에서 제작되었으며, 작중 우선으로 사용되는 언어가 영어가 아닌 장편영화'로 규정하고 있다. 이러한 조건을 만족하는 영화에 대해 아카데미 측은 국제영화상 부문 상을 수여하고 있다. 허나 외국어영화상 후보 작품이 로스앤젤레스군 내 상업 개봉이 이뤄졌다는 조건을 충족하고 각 부문의 특수한 규정을 지킨다면 다른 부문에서도 충분히 수상할 수가 있다. 추가로 미국에서 제작된 외국어 영화는 국제영화상 부문 후보로 오를 수 없으나 다른 부문은 얼마든지 가능하다. \\n\\n2020년 기준으로 외국어영화상을 수상한 작품 가운데 다른 아카데미상 부문에서도 수상한 작품은 총 26편이다. 그 중 최다수상 기록을 가진 작품은 총 세 편으로, 1985년 스웨덴 영화 <화니와 알렉산더>, 2000년 중화민국 영화 <와호장룡>, 2020년 대한민국 영화 <기생충>이며 이들 모두 외국어영화상을 비롯해 4개 부문에서 수상하였다. 최다 후보지명 작품으로는 2000년의 <와호장룡>과 2018년 <로마>가 있으며 10회 노미네이트를 기록하였다. \\n\\n미국 영화예술과학 아카데미는 1956년부터 정기부문인 '외국어영화상' 부문을 신설, 매해 시상식마다 각국 영화업계로부터 출품작을 받고 있다. 이러한 출품작에 대한 심사 과정과 평가는 외국어영화상 위원회가 전담하며, 비밀 투표를 통해 본선에 올릴 최종 후보작 다섯 편을 추린다 1956년 정기부문 신설 이전에는 출품작 심사나 경쟁 과정 없이, 아카데미 운영위 측에서 매년마다 미국 내에서 개봉한 최고의 외국어 영화상을 한 편 뽑아 '명예상'을 수여하는 방식으로 진행했다.\n",
      "        Question:외국어영화상 위원회에서 최종 후보 다섯 편을 추리는 방법은?\n",
      " Meta Language:단편적인 정보를 조합하여 답을 도출.\n",
      "Act:단편적인 정보를\n",
      "answer : ['비밀 투표']\n",
      "---------------------------------------------------------------------\n",
      "pred : [|system|] You are EXAONE model from LG AI Research, a helpful assistant. [|endofturn|]\n",
      "        [|user|] Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that \"없음\", don't try to make up an answer. Please answer in short answer. Keep the answer as concise.\n",
      "        석탄기(石炭紀)는 고생대 가운데 다섯 번째 기이며, 데본기와 페름기 사이 시대다. 시작은 3억5920만 년 전, 끝은 2억9900만 년 전으로 여기고 있다. 다른 지질 시대 구분에서와 마찬가지로 암석에서 시작과 끝은 잘 정의했으나 그 정확한 시기에 관해 500만 년에서 1000만 년의 불확실성이 있다. 석탄기라는 명칭은 영국과 서유럽 이 시대 지층에서 방대한 양의 석탄층을 산출했기 때문에 붙인 이름이다. 북아메리카 지층에서는 이 시대 전기 1/3 동안은 미시시피기, 이후는 펜실베이니아기로 구분한다. 침엽수가 이 시기에 출현하고 번성했다. 이때 하루살이를 비롯한 고시하강과 원시 신시하강의 곤충이 생겼다. 이 시기까지 남반구 대륙들은 아직 합쳐져서 초대륙 곤드와나를 이루고 있었다. 북아메리카와 유럽이 합쳐져 있었던 로라시아 대륙이 곤드와나 대륙에 합체하고 있었는데, 충돌 부분은 지금 북아메리카의 동부였다. 한편 이 때에 동유라시아판은 유럽판과 합쳐지면서 우랄 산맥을 형성했다. 중생대 초대륙 판게아의 대부분은 이때 합쳐졌으나 현재의 동아시아 부분은 아직 떨어져 있었다. 유럽과 동부 북아메리카의 탄산염암은 석회암, 사암, 셰일의 순서를 잘 반복하고 있다. 북아메리카에는 석탄기 초기는 대부분 해양에서 퇴적한 석회암이다. 이 때문에 북미 석탄기는 미시시피기와 펜실베이니아기 둘로 나뉜다. 석탄기 석탄층은 산업혁명 기간 동안 연료로 사용했으며, 아직도 경제적으로 중요하다.\n",
      "        Question:석탄기 다음 시대는?\n",
      " Meta Language: Question:석탄기 다음 시대는?\n",
      "Activity: Question:석탄기 다음 시대는?\n",
      "answer : ['페름기']\n",
      "---------------------------------------------------------------------\n",
      "pred : [|system|] You are EXAONE model from LG AI Research, a helpful assistant. [|endofturn|]\n",
      "        [|user|] Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that \"없음\", don't try to make up an answer. Please answer in short answer. Keep the answer as concise.\n",
      "        대한민국에서 타코벨은 피자헛을 운영하는 업체에서 1991년에 도입 했으나, 당시 기준으로는 지나치게 고가 정책을 고수한 점이 약점으로 작용, 피자헛과 달리 큰 성과를 내지 못하고 몇 년만에 대한민국 시장에서 철수했다. 2010년, 특수목적회사 M2G는 대한민국에 타코벨을 재도입한다고 발표, 7월 11일 서울 이태원에 타코벨 점포가 문을 열었다. 새로 문을 연 대한민국 내의 타코벨은 과거와 달리 셀프서비스 형식의 패스트푸드 형태이며, 저가 메뉴 중심으로 운영한다. 하지만 M2G 또한 운영이 지지부진하자 미국 본사에서 새로운 파트너를 찾게되었고, 2014년 12월 12일에 LG계열의 아워홈이 국내 프랜차이즈를 계약하고 영등포 타임스퀘어에 1호점을 출점하였다. 이에 M2G는 미국 타코벨이 의도적으로 영업을 방해하고서는 사업부진을 구실로 대기업 아워홈과 복수계약을 맺었다고 반발한 반면, 미국 본사에서는 애초에 독점 계약이 아니었으며, M2G가 타코벨의 기준에 맞지 않는 품질과 서비스를 선보였다며 M2G가 이를 개선할 경우 사업을 지원하겠다고 밝혔다. M2G는 타코벨과 아워홈을 공정거래법 위반으로 공정거래위원회에 제소한 상태다.\n",
      "        Question:타코벨과 달리 대한민국에서 많은 성과를 달성했던 업체는?\n",
      " Meta Language Framework의 도움을 받아 질문에 답하였습니다.\n",
      "Parallel World Framework의 도움을 받아 질문에\n",
      "answer : ['피자헛']\n",
      "---------------------------------------------------------------------\n",
      "pred : [|system|] You are EXAONE model from LG AI Research, a helpful assistant. [|endofturn|]\n",
      "        [|user|] Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that \"없음\", don't try to make up an answer. Please answer in short answer. Keep the answer as concise.\n",
      "        트리부스 민회(Comitia Tributa)는 로마 공화국 사회에서 행정과 관련된 민회 중 하나이다. 부족 의회로 부족들이 소집한 모든 로마 시민들로 구성된 집합체였다. 로마 공화정 시기에 시민은 35개의 부족을 기반으로 조직되어 있었다. 4개의 로마 시내 부족과 31개의 시 외곽 부족이 있었다. 부족들은 입법, 사법, 선거와 관련된 문제에 투표하기 위해 부족 의회를 소집했다. 각 부족은 별도로 분리된 투표를 했다. 각 부족마다 다수결 득표에 의한 결정이 내려졌으며, 각 부족이 보유한 선거인 수와 관계없이 결정은 1표로 계산되었다. 일단 대다수의 부족이 주어진 방법과 같은 방법으로 투표하면 투표가 끝나고 문제가 결정되었다. \\n\\n참가 자격은 로마 시민권을 가진 상류층인 파트리키와 평민 계급인 플레브스가 동시에 국정에 참여했다. 대부분의 인원이 로마 시내에 있는 4개의 선거구(트리부스 = 본래 ‘부족’이라는 의미)에 집중되어 있으며, 여기에서 투표 결과를 좌우했다. 또한 켄투리아 민회처럼 각자의 시민들이 투표권을 가지고 있는 것은 아니었고, 각 선거구에 하나의 투표권이 주어졌다. 트리부스 민회는 포룸 로마눔에서 열린 수석 조영관, 재무관, 거기에 트리뷴 밀리툼이라는 군단 사령관의 선거를 결정했다. 또한 루키우스 코르넬리우스 술라에 의한 개혁까지 트리부스 민회에서 재판도 진행되었다.\n",
      "        Question:1표로 간주하기 위한 의견 수렴 방식은?\n",
      " IP: 119.185.140.130\n",
      "Reverse Translation:\n",
      "Answer\n",
      "answer : ['다수결']\n",
      "---------------------------------------------------------------------\n",
      "pred : [|system|] You are EXAONE model from LG AI Research, a helpful assistant. [|endofturn|]\n",
      "        [|user|] Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that \"없음\", don't try to make up an answer. Please answer in short answer. Keep the answer as concise.\n",
      "        이날 18일 저녁 북한군은 전차 5대를 포함한 보전협동 공격을 기도하였다. 북한군은 다부동 접근로에 미군이 투입된 것을 전혀 모르고 사격을 가하며 접근하고 있었다. 얼마 후 적 전차 2대가 미 제27연대의 F중대의 방어진지 안ᄑ으로 접근하였을 때 그 중 1대가 3.5인치 로켓포탄에 맞아 화염에 휩싸이자 다른 한 대의 승무원은 전차를 버리고 도주하였다. 후속하던 적 전차 3대도 미군의 치열한 포병사격을 받고 후퇴하였다. 그로부터 2시간 뒤 적은 다시 공격을 하였으나 미군의 탄막사격에 의해 격퇴되었다.\\n\\n그렇지만 8월 18일 새벽에는 가산에서 침투한 일부의 적이 사격한 박격포 포탄이 대구역 부근에 떨어지자 대구의 위기는 더욱 고조되었다. 이 출격으로 이날 정부가 부산으로 이동하고 피난령이 하달되어 대구는 일대 혼란에 휩싸였다. 그러나 조병옥 내무부 장관이 경찰과 함께 가두에 나서피난령을 취소하고 가까스로 민심을 수습하였다.\\n\\n다음날 19일은 소강상태로 하루를 보내고 8월 20일 주간에는 유엔공군의 대대적인 항공폭격이 실시되었다. 그러나 전투기를 유도하는 전방항공통제관이 제11연대 예하 대대에는 파견되지 않았던 탓으로 674고지의 아군이 오폭으로 많은 피해를 입었고, 또 448고지 일대에도 폭격하였기 때문에 이곳에 배치된 제11연대 제1대대가 진목정으로 철수하였다가 이날 저녁에 다시 그 고지로 이동하였다.\\n\\n이윽고 8월 20일 17:00시경에는 적의 포탄이 다부동 접근로로 연신되고 뒤이어 적 전차가 보병과 함께 은밀하게 접근하여 왔다. 이 때 미군은 침묵을 지키다가 그들이 진전 지뢰지대에 이르러 지뢰를 제거하자 탄막사격으로 적을 격퇴하였다. 이날 밤 02:00~03시 사이에 1개 대대 규모의 북한군이 접근하였다. 아군은 그들은 진전 50미터까지 바싹 유인한 다음 기습사격을 가하고 수류탄으로 살상지대를 형성하여 격멸하였다.\n",
      "        Question:8월 18일 대구역 근처에 박격포를 쏜 적군은 어디에서 침투했나?\n",
      " CS(단일답):가산[|endofturn|]\n",
      "answer : ['가산']\n",
      "---------------------------------------------------------------------\n",
      "pred : [|system|] You are EXAONE model from LG AI Research, a helpful assistant. [|endofturn|]\n",
      "        [|user|] Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that \"없음\", don't try to make up an answer. Please answer in short answer. Keep the answer as concise.\n",
      "        퇴임에서 볼드윈의 세월은 조용하였다. 네빌 체임벌린이 사망하면서 전쟁 이전의 유화 정책에서 볼드윈의 지각된 부분은 제2차 세계 대전이 일어난 동안과 그 후에 그를 인기없는 인물로 만들었다. 신문의 캠페인은 그를 전쟁 생산에 자신의 시골 저택의 철문을 기부하지 않은 것으로 사냥하였다. 전쟁이 일어난 동안 윈스턴 처칠은 에이먼 데 벌레라의 아일랜드의 지속적인 중립을 향한 더욱 힘든 경향을 취하는 영국의 조언에 그를 단 한번 상담하였다.\\n\\n1945년 6월 부인 루시 여사가 사망하였다. 이제 볼드윈 자신은 관절염을 겪어 걸어다는 데 지팡이가 필요하였다. 조지 5세의 동상의 공개식에 1947년 런던에서 자신의 최종 공개적인 출연을 이루었다. 관중들은 전직 총리를 알아주어 그를 응원하였으나 이 당시 볼드윈은 귀머거리였고, 그들에게 \"당신들은 나를 야유합니까?\"라고 의문하였다. 1930년 케임브리지 대학교의 총장으로 만들어진 그는 1947년 12월 14일 80세의 나이에 우스터셔주 스투어포트온세번 근처 애슬리홀에서 수면 중 자신의 사망까지 이 수용력에 지속하였다. 그는 화장되었고, 그의 재는 우스터 대성당에 안치되었다.\n",
      "        Question:볼드윈이 \"당신들은 나를 야유합니까?\"라는 말을 한 연도는?\n",
      " IP: 1947년 12월 14일에 사망하였고, 그의\n",
      "answer : ['1947년']\n",
      "---------------------------------------------------------------------\n",
      "pred : [|system|] You are EXAONE model from LG AI Research, a helpful assistant. [|endofturn|]\n",
      "        [|user|] Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that \"없음\", don't try to make up an answer. Please answer in short answer. Keep the answer as concise.\n",
      "        2002년 솔트레이크시티 올림픽 대회를 앞두고 푸에르토리코 봅슬레이 팀이 꾸려졌지만 미카엘 곤살레스 선수가 푸에르토리코 올림픽위원회의 기준을 충족하지 못했다는 이유로 선수자격이 박탈되면서 본 경기에는 나가지 못했다. 이후 푸에르토리코 올림픽위원회가 푸에르토리코 겨울스포츠 연맹의 등록을 취소하면서 푸에르토리코 선수들은 동계올림픽에 출전하지 못하게 되었다. 특히, 알파인스키 선수 크리스티나 크로네는 2010년 동계 올림픽과 2014년 동계 올림픽 대회에 출전 자격을 얻었으나 두 번 모두 푸에르토리코 올림픽 위원회가 출전자격 부여 승인조차 불허하였고, 두 대회 모두 푸에르토리코는 불참하게 되었다. \\n\\n하지만 2017년 12월, 푸에르토리코 올림픽 위원회가 동계스포츠 연맹 측에 6개월 시한의 임시 회원자격을 부여하면서 2018년 동계 올림픽부터 푸에르토리코가 참가할 수 있게 되었다. 이후 찰스 플래허티 선수가 푸에르토리코 국가대표로 발탁되어 이번 대회에 출전할 예정이다. 플래허티 선수는 미국 본토에서 태어났으나 푸에르토리코에서 수년간 거주하였기에 푸에르토리코를 대표해 경기에 나갈 수 있게 되었다.\n",
      "        Question:푸에르토리코의 미카엘 곤살레스 선수가 참가하고자 했던 동계올림픽의 종목은?\n",
      " IP: 118.191.184.144\n",
      "Programming bot. Answer\n",
      "answer : ['봅슬레이']\n",
      "---------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "val_dataset = val_dataset.shuffle(seed=104)\n",
    "answers = [answer['text'] for answer in val_dataset['answers']]\n",
    "test = Dataset.from_dict({'context':val_dataset['context'][:10], 'question':val_dataset['question'][:10], 'answers':answers[:10]})\n",
    "\n",
    "for data in test:\n",
    "        prompt = f'''[|system|] You are EXAONE model from LG AI Research, a helpful assistant. [|endofturn|]\n",
    "        [|user|] Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that \"없음\", don't try to make up an answer. Please answer in short answer. Keep the answer as concise.\n",
    "        {data['context']}\n",
    "        Question:{data['question']}'''\n",
    "\n",
    "        input_ids = tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        output = model.generate(\n",
    "            input_ids['input_ids'].to(\"cuda\"),\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            max_new_tokens=25\n",
    "        )\n",
    "        print(f\"pred : {tokenizer.decode(output[0])}\")\n",
    "        print(f\"answer : {data['answers']}\")\n",
    "        print('---------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "val_dataset = val_dataset.shuffle(seed=104)\n",
    "answers = [answer['text'] for answer in val_dataset['answers']]\n",
    "test = Dataset.from_dict({'context':val_dataset['context'][:10], 'question':val_dataset['context'][:10], 'answers':answers[:10]})\n",
    "\n",
    "for data in test:\n",
    "        prompt = f'''[|system|] You are EXAONE model from LG AI Research, a helpful assistant. [|endofturn|]\n",
    "        [|user|] Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that \"없음\", don't try to make up an answer. Please answer in short answer. Keep the answer as concise.\n",
    "        {data['context']}\n",
    "        Question:{data['question']}\n",
    "        [|assistant|] [|endofturn|]'''\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \n",
    "            \"content\": \"You are EXAONE model from LG AI Research, a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        '''input_ids = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )'''\n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        output = model.generate(\n",
    "            input_ids.to(\"cuda\"),\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            max_new_tokens=128\n",
    "        )\n",
    "        print(f\"pred : {tokenizer.decode(output[0])}\")\n",
    "        print(f\"answer : {data['answers']}\")\n",
    "        print('---------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExaoneForCausalLM(\n",
       "  (transformer): ExaoneModel(\n",
       "    (wte): Embedding(102400, 4096, padding_idx=0)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-31): 32 x ExaoneBlock(\n",
       "        (ln_1): ExaoneRMSNorm()\n",
       "        (attn): ExaoneAttention(\n",
       "          (attention): ExaoneSdpaAttention(\n",
       "            (rotary): ExaoneRotaryEmbedding()\n",
       "            (k_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (v_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (q_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (out_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_2): ExaoneRMSNorm()\n",
       "        (mlp): ExaoneGatedMLP(\n",
       "          (c_fc_0): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=14336, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (c_fc_1): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=14336, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (c_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=14336, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (act): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): ExaoneRMSNorm()\n",
       "    (rotary): ExaoneRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=102400, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAONE_TEMP = '''[|system|] You are EXAONE model from LG AI Research, a helpful assistant. [|endofturn|]\n",
    "[|user|] Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that \"없음\", don't try to make up an answer. Please answer in short answer. Keep the answer as concise.\n",
    "{}\n",
    "Question:{}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = '스카버러 남쪽과 코보콘그 마을의 철도 노선이 처음 연장된 연도는?'\n",
    "context = '요크 카운티 동쪽에 처음으로 여객 열차 운행이 시작한 시점은 1868년 토론토 & 니피싱 철도의 설립 인가가 떨어졌을 때였다. 스카버러 남쪽과 코보콘크 마을을 잇는 철도 노선 공사가 시작되었고 1871년 6월에 억스브릿지까지 철도가 완공되었다. 이 노선은 이후 1871년 11월에 캐닝턴까지 연장되었고 1872년 11월에 코보콘크까지 완공되었다. \\n\\n이 노선의 철로 궤간은 1067mm로 협궤 노선이였다. 목재와 장작 수요가 선로 용량이 넘칠 정도로 많았지만 1870년대에 다른 철도와 마찬가지로 경제적 난황을 이기 못해 수익이 줄어들었다. 투자자들은 수익이 줄어든 이유로 철도가 협궤로 지어져서 다른 표준궤 노선과 연계하여 화물 운송을 할 수 없는 점을 들었다 \\n\\n1881년 7월, 미들랜드 철도가 토론토 & 니피싱 철도를 인수하였고 이 노선은 표준궤로 전환되었다. 미들랜드 철도의 영향은 스카버러에서도 볼 수 있는데 노선 동쪽을 따라 나란히 달리는 미들랜드 애비뉴가 이 철도 회사의 이름을 따서 지어졌다. 1884년, 미들랜드 철도는 그랜드 트렁크 철도에 인수되었고 이는 이후 캐나다 내셔널 철도에 다시 인수되었다. CN은 이 노선을 억스브릿지 선으로 명명하였다\\n\\n억스브릿지 선을 따라 달리는 여객 철도는 통근객을 위한 열차가 아니였다. 처음에는 론빌에 있는 미들랜드정션역과 토론토를 잇는 열차가 두 대씩 양방향으로 운행했고 한 대는 북동쪽으로 코보콘크까지 운행하였다. 토론토 북동쪽에 있던 시골 마을의 수요는 저조하기만 하였으며 20세기에 들어서서 도로 개량과 자동차 보편화로 쇠락의 길을 걷게 되었다\\n\\n1955년에는 코보콘크에서 억스브릿지까지 여객 열차 운행이 중단되었으며 30년 뒤 선로 또한 폐선되었다. 1963년에는 억스브릿지까지 이어지는 여객열차 운행이 중단되고 CN 열차는 토론토 유니언역에서 마컴으로 가는 열차를 5시 20분에 딱 한 대 운행하였다. 토론토로 돌아오는 열차는 존재하지 않았다'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[|system|] You are EXAONE model from LG AI Research, a helpful assistant. [|endofturn|]\n",
      "[|user|] Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that \"없음\", don't try to make up an answer.\n",
      "요크 카운티 동쪽에 처음으로 여객 열차 운행이 시작한 시점은 1868년 토론토 & 니피싱 철도의 설립 인가가 떨어졌을 때였다. 스카버러 남쪽과 코보콘크 마을을 잇는 철도 노선 공사가 시작되었고 1871년 6월에 억스브릿지까지 철도가 완공되었다. 이 노선은 이후 1871년 11월에 캐닝턴까지 연장되었고 1872년 11월에 코보콘크까지 완공되었다. \n",
      "\n",
      "이 노선의 철로 궤간은 1067mm로 협궤 노선이였다. 목재와 장작 수요가 선로 용량이 넘칠 정도로 많았지만 1870년대에 다른 철도와 마찬가지로 경제적 난황을 이기 못해 수익이 줄어들었다. 투자자들은 수익이 줄어든 이유로 철도가 협궤로 지어져서 다른 표준궤 노선과 연계하여 화물 운송을 할 수 없는 점을 들었다 \n",
      "\n",
      "1881년 7월, 미들랜드 철도가 토론토 & 니피싱 철도를 인수하였고 이 노선은 표준궤로 전환되었다. 미들랜드 철도의 영향은 스카버러에서도 볼 수 있는데 노선 동쪽을 따라 나란히 달리는 미들랜드 애비뉴가 이 철도 회사의 이름을 따서 지어졌다. 1884년, 미들랜드 철도는 그랜드 트렁크 철도에 인수되었고 이는 이후 캐나다 내셔널 철도에 다시 인수되었다. CN은 이 노선을 억스브릿지 선으로 명명하였다\n",
      "\n",
      "억스브릿지 선을 따라 달리는 여객 철도는 통근객을 위한 열차가 아니였다. 처음에는 론빌에 있는 미들랜드정션역과 토론토를 잇는 열차가 두 대씩 양방향으로 운행했고 한 대는 북동쪽으로 코보콘크까지 운행하였다. 토론토 북동쪽에 있던 시골 마을의 수요는 저조하기만 하였으며 20세기에 들어서서 도로 개량과 자동차 보편화로 쇠락의 길을 걷게 되었다\n",
      "\n",
      "1955년에는 코보콘크에서 억스브릿지까지 여객 열차 운행이 중단되었으며 30년 뒤 선로 또한 폐선되었다. 1963년에는 억스브릿지까지 이어지는 여객열차 운행이 중단되고 CN 열차는 토론토 유니언역에서 마컴으로 가는 열차를 5시 20분에 딱 한 대 운행하였다. 토론토로 돌아오는 열차는 존재하지 않았다\n",
      "Question:스카버러 남쪽과 코보콘그 마을의 철도 노선이 처음 연장된 연도는?\n"
     ]
    }
   ],
   "source": [
    "promt = EXAONE_TEMP.format(context, question)\n",
    "print(promt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 651])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(promt, return_tensors=\"pt\")['input_ids']\n",
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[|system|] You are EXAONE model from LG AI Research, a helpful assistant. [|endofturn|]\n",
      "[|user|] Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that \"없음\", don't try to make up an answer.\n",
      "요크 카운티 동쪽에 처음으로 여객 열차 운행이 시작한 시점은 1868년 토론토 & 니피싱 철도의 설립 인가가 떨어졌을 때였다. 스카버러 남쪽과 코보콘크 마을을 잇는 철도 노선 공사가 시작되었고 1871년 6월에 억스브릿지까지 철도가 완공되었다. 이 노선은 이후 1871년 11월에 캐닝턴까지 연장되었고 1872년 11월에 코보콘크까지 완공되었다. \n",
      "\n",
      "이 노선의 철로 궤간은 1067mm로 협궤 노선이였다. 목재와 장작 수요가 선로 용량이 넘칠 정도로 많았지만 1870년대에 다른 철도와 마찬가지로 경제적 난황을 이기 못해 수익이 줄어들었다. 투자자들은 수익이 줄어든 이유로 철도가 협궤로 지어져서 다른 표준궤 노선과 연계하여 화물 운송을 할 수 없는 점을 들었다 \n",
      "\n",
      "1881년 7월, 미들랜드 철도가 토론토 & 니피싱 철도를 인수하였고 이 노선은 표준궤로 전환되었다. 미들랜드 철도의 영향은 스카버러에서도 볼 수 있는데 노선 동쪽을 따라 나란히 달리는 미들랜드 애비뉴가 이 철도 회사의 이름을 따서 지어졌다. 1884년, 미들랜드 철도는 그랜드 트렁크 철도에 인수되었고 이는 이후 캐나다 내셔널 철도에 다시 인수되었다. CN은 이 노선을 억스브릿지 선으로 명명하였다\n",
      "\n",
      "억스브릿지 선을 따라 달리는 여객 철도는 통근객을 위한 열차가 아니였다. 처음에는 론빌에 있는 미들랜드정션역과 토론토를 잇는 열차가 두 대씩 양방향으로 운행했고 한 대는 북동쪽으로 코보콘크까지 운행하였다. 토론토 북동쪽에 있던 시골 마을의 수요는 저조하기만 하였으며 20세기에 들어서서 도로 개량과 자동차 보편화로 쇠락의 길을 걷게 되었다\n",
      "\n",
      "1955년에는 코보콘크에서 억스브릿지까지 여객 열차 운행이 중단되었으며 30년 뒤 선로 또한 폐선되었다. 1963년에는 억스브릿지까지 이어지는 여객열차 운행이 중단되고 CN 열차는 토론토 유니언역에서 마컴으로 가는 열차를 5시 20분에 딱 한 대 운행하였다. 토론토로 돌아오는 열차는 존재하지 않았다\n",
      "Question:스카버러 남쪽과 코보콘그 마을의 철도 노선이 처음 연장된 연도는?\n",
      "|\n",
      "|\n",
      "1871년 11월\n",
      "|\n",
      "|\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(\n",
    "    input_ids.to(\"cuda\"),\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    num_beams = 1,\n",
    "    max_new_tokens=20\n",
    ")\n",
    "print(tokenizer.decode(output[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_sh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
