{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import nltk\n",
    "#nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk(\"./resources/data/train_dataset/\")\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/mrc-nlp-08/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, EvalPrediction, TrainingArguments, DataCollatorWithPadding, EarlyStoppingCallback, AutoModelForSeq2SeqLM\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gogamza/kobart-base-v2\")\n",
    "model = BartForConditionalGeneration.from_pretrained(\"gogamza/kobart-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '미국 상원',\n",
       " 'context': '미국 상의원 또는 미국 상원(United States Senate)은 양원제인 미국 의회의 상원이다.\\\\n\\\\n미국 부통령이 상원의장이 된다. 각 주당 2명의 상원의원이 선출되어 100명의 상원의원으로 구성되어 있다. 임기는 6년이며, 2년마다 50개주 중 1/3씩 상원의원을 새로 선출하여 연방에 보낸다.\\\\n\\\\n미국 상원은 미국 하원과는 다르게 미국 대통령을 수반으로 하는 미국 연방 행정부에 각종 동의를 하는 기관이다. 하원이 세금과 경제에 대한 권한, 대통령을 포함한 대다수의 공무원을 파면할 권한을 갖고 있는 국민을 대표하는 기관인 반면 상원은 미국의 주를 대표한다. 즉 캘리포니아주, 일리노이주 같이 주 정부와 주 의회를 대표하는 기관이다. 그로 인하여 군대의 파병, 관료의 임명에 대한 동의, 외국 조약에 대한 승인 등 신속을 요하는 권한은 모두 상원에게만 있다. 그리고 하원에 대한 견제 역할(하원의 법안을 거부할 권한 등)을 담당한다. 2년의 임기로 인하여 급진적일 수밖에 없는 하원은 지나치게 급진적인 법안을 만들기 쉽다. 대표적인 예로 건강보험 개혁 당시 하원이 미국 연방 행정부에게 퍼블릭 옵션(공공건강보험기관)의 조항이 있는 반면 상원의 경우 하원안이 지나치게 세금이 많이 든다는 이유로 퍼블릭 옵션 조항을 제외하고 비영리건강보험기관이나 보험회사가 담당하도록 한 것이다. 이 경우처럼 상원은 하원이나 내각책임제가 빠지기 쉬운 국가들의 국회처럼 걸핏하면 발생하는 의회의 비정상적인 사태를 방지하는 기관이다. 상원은 급박한 처리사항의 경우가 아니면 법안을 먼저 내는 경우가 드물고 하원이 만든 법안을 수정하여 다시 하원에 되돌려보낸다. 이러한 방식으로 단원제가 빠지기 쉬운 함정을 미리 방지하는 것이다.날짜=2017-02-05',\n",
       " 'question': '대통령을 포함한 미국의 행정부 견제권을 갖는 국가 기관은?',\n",
       " 'id': 'mrc-1-000067',\n",
       " 'answers': {'answer_start': [235], 'text': ['하원']},\n",
       " 'document_id': 18293,\n",
       " '__index_level_0__': 42}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3952, 240)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3952/3952 [00:02<00:00, 1501.05 examples/s]\n",
      "Map: 100%|██████████| 240/240 [00:00<00:00, 1507.18 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Train preprocessing / 전처리를 진행합니다.\n",
    "def prepare_features(examples):\n",
    "    # truncation과 padding(length가 짧을때만)을 통해 toknization을 진행하며, stride를 이용하여 overflow를 유지합니다.\n",
    "    # 각 example들은 이전의 context와 조금씩 겹치게됩니다.\n",
    "    labels = [answers['text'][0] for answers in examples['answers']]\n",
    "    tokenized_examples = tokenizer(\n",
    "        text=examples['question'],\n",
    "        text_pair=examples['context'],\n",
    "        text_target=labels,\n",
    "        padding=\"max_length\",\n",
    "        truncation=\"only_second\",    # context만 truncate하기\n",
    "        max_length=512,\n",
    "        stride=128, # 이전 chunk와 overlap되는 부분을 두어 긴 문서를 처리할 때 유용. 모델이 더 나은 임베딩을 하도록 도와 QA에 유용.\n",
    "        return_overflowing_tokens=True,\n",
    "    )\n",
    "    del tokenized_examples['token_type_ids']\n",
    "    # labels 확장\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    tokenized_examples[\"labels\"] = [tokenized_examples['labels'][i] for i in sample_mapping]\n",
    "    \n",
    "    return tokenized_examples\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "            prepare_features,\n",
    "            batched=True,\n",
    "            num_proc=None,\n",
    "            remove_columns=train_dataset.column_names,\n",
    "            load_from_cache_file=not False)\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "            prepare_features,\n",
    "            batched=True,\n",
    "            num_proc=None,\n",
    "            remove_columns=val_dataset.column_names,\n",
    "            load_from_cache_file=not False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 5107\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 307\n",
       " }))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/mrc-nlp-08/.venv/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"squad\")\n",
    "\n",
    "def compute_metrics(p: EvalPrediction): #EvalPrediction 구조 | predictions: 모델의 예측값, label_ids: 실제 정답 레이블\n",
    "    predictions = p.predictions\n",
    "    references = p.label_ids\n",
    "    \n",
    "    if np.array(predictions).ndim == 3:\n",
    "        predictions = np.argmax(predictions, axis=-1)\n",
    "        \n",
    "    predictions = [{'prediction_text': pred, 'id': str(i)} for i, pred in enumerate(predictions)]\n",
    "    references = [{'answers': {'answer_start': [], 'text': [label]}, 'id': str(i)} for i, label in enumerate(references)]\n",
    "\n",
    "    result = metric.compute(predictions=predictions, references=references)\n",
    "    result['eval_exact_match'] = result['exact_match']\n",
    "    del result['exact_match']\n",
    "    result['eval_f1'] = result['f1']\n",
    "    del result['f1']\n",
    "    return result\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ckpt/bart\",num_train_epochs = 25,\n",
    "    save_strategy='epoch', evaluation_strategy=\"epoch\",\n",
    "    remove_unused_columns=False, optim=\"adamw_hf\",\n",
    "    per_device_train_batch_size=16, weight_decay=0.01,\n",
    "    save_total_limit = 1, load_best_model_at_end = True)\n",
    "\n",
    "    #warmup_steps=1000,\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8 if training_args.fp16 else None)\n",
    "\n",
    "# Trainer 초기화\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    "    #callbacks = [EarlyStoppingCallback(early_stopping_patience=10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/mrc-nlp-08/.venv/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2719' max='8000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2719/8000 50:41 < 1:38:32, 0.89 it/s, Epoch 8.49/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.013695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.014684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.015471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.016835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.016778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.016847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.017352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.018456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mrc-nlp-08/.venv/lib/python3.10/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mrc-nlp-08/.venv/lib/python3.10/site-packages/transformers/trainer.py:2393\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2394\u001b[0m ):\n\u001b[1;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(context, question):\n",
    "    # 모델을 평가 모드로 설정\n",
    "    trainer.model.eval()\n",
    "\n",
    "\n",
    "    # 입력 텍스트를 토크나이즈\n",
    "    inputs = tokenizer(\n",
    "            text=question,\n",
    "            text_pair=context,\n",
    "            padding=\"max_length\",\n",
    "            truncation=\"only_second\",    # context만 truncate하기\n",
    "            max_length=384,\n",
    "            stride=128, # 이전 chunk와 overlap되는 부분을 두어 긴 문서를 처리할 때 유용. 모델이 더 나은 임베딩을 하도록 도와 QA에 유용.\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "    del inputs['token_type_ids']\n",
    "\n",
    "    # GPU 사용 시 입력을 GPU로 이동\n",
    "    inputs = {k: torch.tensor(v).to(trainer.model.device) for k, v in inputs.items()}\n",
    "\n",
    "    outputs = model.generate(**inputs, max_length=30, num_beams=5)\n",
    "    '''# inference 수행\n",
    "    with torch.no_grad():\n",
    "        outputs = trainer.model(**inputs)'''\n",
    "    # 결과 처리\n",
    "    #predictions = outputs.argmax(-1)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: 'ᄏᄏᄏ'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_474890/3863879182.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = {k: torch.tensor(v).to(trainer.model.device) for k, v in inputs.items()}\n"
     ]
    }
   ],
   "source": [
    "context = ''''ㅋㅋ'은 일반적으로 웃음소리를 표현할 때 사용된다. 통신체에서는 웃는 모습을 직접적으로 보여줄 수 없기 때문에 의성어를 사용하는 경우가 많은데 그 중에서 '킥킥', '큭큭'을 초성체로 바꾸어 사용한 것이 'ㅋㅋ'에 해당한다. 이에 대해 2001년 연구에서는 \"자음만 가지고 표시하여 통신상의 재미를 더하기 위한 방법으로 사용되고 있다\"고 해석한 바 있다. \\n\\nㅋ자를 웃음소리를 의미하는 것으로 사용할 때, 'ㅋ'자 한 개에서 'ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ...'하는 식으로 열 타 이상 한 줄 넘게까지 쓰이기도 한다 ㅋ자를 얼마나 많이 썼느냐에 따라 그 느낌과 의미는 달라진다. 신조어를 전문으로 연구하는 이재현 문화평론가는 《한국일보》의 기고글에서 그 종류와 느낌이 대략 다음과 같다고 보았다\\n* ㅋ - 일반적으로 무심하게 동의하려는 상황에서 쓰인다. 다른 말끝에 붙였을 때, 이를테면 '그래ㅋ', '좋네ㅋ', '올ㅋ' 등은 웃음 자체라기보다는 입꼬리가 아주 살짝 올라간 상황이라는 느낌을 준다. 의성어로는 \"큭\" 혹은 \"킥\"으로 해석될 수 있다. 한편 '뭐해?ㅋ', '미안ㅋ'의 경우 'ㅋ'는 상투적인 군말의 역할을 한다.\\n* ㅋㅋ - 위의 것보다 ㅋ자가 하나가 더 많으나 오히려 형식적이라는 느낌을 준다. 좋게는 '그렇군'을 뜻하거나, 조금 나쁘게는 대화상에서 의례적으로 추임새를 넣고 있다는 느낌을 준다. 'ㅋㅋ'은 미혼남녀들이 가장 싫어하는 성의 없는 메신저 말투로 선정되기도 했다. 다만 회사 상사에게 대답하는 경우 같은 공적인 상황에서는 '네ㅋㅋ'와 같은 대답은 가벼워 보인다는 조사도 있었다. \\n* ㅋㅋㅋ - 비교적 중립적이면서도 가장 많이 사용되는 용법이다. 웃는 감정을 그대로 표현한다는 뉘앙스를 갖는다.\\n* ㅋㅋㅋㅋ... - 여기서부터는 \"정말 웃긴다\"라는 반응을 나름 정성들여서 표현하는 느낌을 준다. 네 개부터 그 이상은 의미의 함축과 정서의 강도가 거의 같으며, 이런 점에서 'ㅋㅋㅋㅋ'는 실질적으로 웃긴다는 것을 표현하는 최소 조건에 해당한다.'''\n",
    "question = '웃는 감정을 그대로 표현한다는 뉘앙스를 갖는 ㅋ의 용법은?'\n",
    "generated_text = inference(context, question)\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: 동계건조\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_474890/3863879182.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = {k: torch.tensor(v).to(trainer.model.device) for k, v in inputs.items()}\n"
     ]
    }
   ],
   "source": [
    "context = '''냉대 동계 소우 기후(冷带冬季少雨氣候)는 쾨펜의 기후 구분에서 냉대 기후에 속하며, 기호는 Dwa, Dwb, Dwc, Dwd로, D는 냉대, w는 동계건조(wintertrocken)을 가리킨다. 트란스 바이칼 기후 라고도 한다. 여름에는 온도가 비교적 높고, 겨울에는 맑은 날씨가 지속되고 약한 바람이 분다. 복사냉각 때문에 매우 쌀쌀한 것이 특징이다. 또 여름에는 계절풍의 영향으로 비가 많이 내린다. 겨울은 여름에 비해 매우 긴 편이다. 기온의 연교차가 매우 크게 나타나는 대륙성 기후가 짙게 나타난다.\\n\\n냉대 동계 소우 기후는 매우 추운 기후로, 농작물을 재배할 수 있는 곳과 재배 수종 및 수가 제한되어 있다. 수수, 옥수수, 봄밀 등을 재배한다. 북부는 타이가 기후와 같이 타이가가 많이 나타나 임업활동을 한다. 숲 또한 냉대 습윤 기후와 비슷하여 남부에는 침엽수와 활엽수가 함께 나타나는 혼합림이, 북부에는 타이가(침엽수림)이 잘 나타난다. 북부는 추위에도 잘 견디는 가늘고 뾰족한 잎을 가진 침엽수가 자라기 유리하기 때문에 타이가가, 남부에는 북부보다 따뜻하여 혼합림이 잘 나타난다. 남부 지역의 극소수에서만 벼 재배가 가능하다.\\n\\n토양은 주로 포드졸이 나타나고, 혼합림에서는 갈색 삼림토가 나타나는데, 포드졸은 유기물이 분해되지는 않았으나, 소금기를 잘 머금지 않고 침엽수림의 특성 때문에 농작물을 재배하는 데에는 적합하지 않다. 그에 반해 갈색 삼림토는 물을 잘 흡수하고 포드졸에 비해 약산성을 띠기 때문에 농작물 재배와 목축에 알맞다.\\n\\n이런 냉대 동계 소우 기후가 나타나는 곳으로는 시베리아 내륙 동부, 중국 화베이 동부 및 둥베이, 대한민국 강원도 내륙 지역과 경기도 및 산악지대, 조선민주주의인민공화국 대부분, 캐나다 및 미국, 네팔의 일부 지역 등이 있으며, 특히 시베리아 동부는 세계의 한극을 만들어 낸다.\\n\\n그래서 이 기후가 냉대 기후의 탈을 쓴 사바나 기후라고 봐야 하는 수준에 이르게 되며, 겨울철에는 강수량이 적어져서 그렇게 건조한 만큼 불씨가 쉽게 일어나는 특성상 산불도 물론 일어나기가 쉬운 기후 대역에 속한다.\\n\\n앞으로 온난화 여파에 따라 이 기후가 사라질 가능성이 있는 지역도 생길 수도 있다. 단적인 예로 대한민국의 수도권, 강원도 등은 현재 Dwa로 분류되나 온대 기후로 바뀔 가능성이 있다.'''\n",
    "question = '냉대 동계 소우 기후의 w는 무엇의 약자인가?'\n",
    "generated_text = inference(context, question)\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: 《사이좋은 가우스전자》\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_474890/3863879182.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = {k: torch.tensor(v).to(trainer.model.device) for k, v in inputs.items()}\n"
     ]
    }
   ],
   "source": [
    "context = '''한국의 웹툰.\\n\\n곽백수가 네이버에서 연재한 웹툰. 회사 생활의 이야기를 주축으로 하는 직장 만화로, 작가의 이전 흥행작인《트라우마》에서도 익히 이름을 알렸던 가우스 그룹#s-1.3이 아예 메인으로 다뤄진다. 따라서, 어떤 의미에서는 트라우마의 스핀오프작이라고도 볼수 있는 셈. 물론, 가우스 그룹의 막나가는 센스는 여전하다. 초기에는 《사이좋은 가우스전자》라는 이름이었지만 어느 순간 '사이좋은'이 빠지고 그냥 《가우스전자》가 되었다.\\n\\n기본적으로《트라우마》와 비슷한 분위기의 개그물이지만 블랙 코미디의 성격이 강하다. 게다가 주제가 주제인지라 회사원 입장에서 공감할 내용들이 많으며, 그런 독자들의 반응은 대개 '내가 웃어도 웃는 게 아니야.' 로 통일되어 있다. 그래도 불편한 진실과 블랙 코미디 사이에서 블랙 코미디 쪽으로 줄타기를 잘 하고 있다. 또한 브리핑 에피소드들을 보면 현 업계의 시류를 잘 집고 있어 작가가 에피소드 소재에 대한 준비를 열심히 한 듯한 느낌이 난다. 그렇기에 특히 회사원들, 즉 어느 정도 연령층이 높은 독자들에게 평이 상당히 좋다. 그래도 상대적으로 악역 포지션인 인물들이 있고 그렇지 않은 인물들이 있지만, 현실 기업과는 비교도 할 수 없을만큼의 훈훈함이 특징.'''\n",
    "question = '《가우스전자》의 원래 제목은 무엇이었나?'\n",
    "generated_text = inference(question, context)\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(context, question, answer):\n",
    "    trainer.model.eval()\n",
    "    # 입력 텍스트를 토크나이즈\n",
    "    inputs = tokenizer(\n",
    "            text=question,\n",
    "            text_pair=context,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,    # context만 truncate하기\n",
    "            max_length=384,\n",
    "            stride=128, # 이전 chunk와 overlap되는 부분을 두어 긴 문서를 처리할 때 유용. 모델이 더 나은 임베딩을 하도록 도와 QA에 유용.\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "    del inputs['token_type_ids']\n",
    "\n",
    "    # GPU 사용 시 입력을 GPU로 이동\n",
    "    inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=30, num_beams=5)\n",
    "\n",
    "    # 결과 처리\n",
    "    #predictions = outputs.argmax(-1)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"pred : {generated_text}\")\n",
    "    print(f\"answer : {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred : 높은 벽들로 감싼 형태\n",
      "answer : {'answer_start': [789], 'text': [\"'달빛 정원'(Moonlight Garden)\"]}\n",
      "pred : 미타케 성\n",
      "answer : {'answer_start': [304], 'text': ['미타케성']}\n",
      "pred : 초나라왕\n",
      "answer : {'answer_start': [1084], 'text': ['제 양왕']}\n",
      "pred : 우천\n",
      "answer : {'answer_start': [406], 'text': ['우천']}\n",
      "pred : 태화관\n",
      "answer : {'answer_start': [115], 'text': ['태화관(서울시 종로구 인사동 소재)']}\n",
      "pred : 김수환\n",
      "answer : {'answer_start': [0], 'text': ['김수환 추기경']}\n",
      "pred : 대집회\n",
      "answer : {'answer_start': [1029], 'text': ['데코행진']}\n",
      "pred : 의상대사\n",
      "answer : {'answer_start': [35], 'text': ['의상대사']}\n",
      "pred : 릿지당\n",
      "answer : {'answer_start': [585], 'text': ['메이저 릿지']}\n",
      "pred : 어리석거나 나쁘다는 식\n",
      "answer : {'answer_start': [468], 'text': ['공정']}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "datasets = load_from_disk('./resources/data/train_dataset')\n",
    "valid_dataset = datasets['validation']\n",
    "\n",
    "valid_dataset = valid_dataset.shuffle(seed=104)\n",
    "answers = [answer['text'] for answer in valid_dataset['answers']]\n",
    "test = Dataset.from_dict({'context':valid_dataset['context'][:10], 'question':valid_dataset['question'][:10], 'answers':valid_dataset['answers'][:10]})\n",
    "\n",
    "# pretrained model 과 tokenizer를 불러오기\n",
    "\n",
    "assert model.config.vocab_size == len(tokenizer), \"Model and tokenizer vocab sizes do not match!\"\n",
    "for data in test:\n",
    "    inference(data['context'], data['question'], data['answers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"KETI-AIR/ke-t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"KETI-AIR/ke-t5-small\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments\n",
    ")\n",
    "max_source_length = 384\n",
    "max_target_length = 16\n",
    "padding = \"max_length\"\n",
    "preprocessing_num_workers = 12\n",
    "num_beams = 5\n",
    "max_train_samples = 3952\n",
    "max_val_samples = 240\n",
    "num_train_epochs = 50\n",
    "train_batch_size = 16\n",
    "eval_batch_size = 8\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "            tokenizer,\n",
    "            model=model,\n",
    "        )\n",
    "\n",
    "# Train preprocessing / 전처리를 진행합니다.\n",
    "def prepare_features(examples):\n",
    "    # truncation과 padding(length가 짧을때만)을 통해 toknization을 진행하며, stride를 이용하여 overflow를 유지합니다.\n",
    "    # 각 example들은 이전의 context와 조금씩 겹치게됩니다.\n",
    "    inputs = [f'question: {q}  context: {c}' for q, c in zip(examples['question'], examples['context'])]\n",
    "    targets = [f'{a[\"text\"][0]}' for a in examples['answers']]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True, return_tensors='pt')\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    model_inputs[\"labels\"][model_inputs[\"labels\"] == tokenizer.pad_token_id] = -100\n",
    "        \n",
    "    return model_inputs\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "            prepare_features,\n",
    "            batched=True,\n",
    "            num_proc=None,\n",
    "            remove_columns=train_dataset.column_names,\n",
    "            load_from_cache_file=not False)\n",
    "\n",
    "print('validation')\n",
    "val_dataset = val_dataset.map(\n",
    "            prepare_features,\n",
    "            batched=True,\n",
    "            num_proc=None,\n",
    "            remove_columns=val_dataset.column_names,\n",
    "            load_from_cache_file=not False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric('squad')\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # decoded_labels is for rouge metric, not used for f1/em metric\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    formatted_predictions = [{\"id\": ex['id'], \"prediction_text\": decoded_preds[i]} for i, ex in enumerate(dataset['validation'].select(range(max_val_samples)))]\n",
    "    references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in dataset['validation'].select(range(max_val_samples))]\n",
    "\n",
    "    result = metric.compute(predictions=formatted_predictions, references=references)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./ckpt/t5',\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    optim=\"adamw_hf\",\n",
    "    warmup_steps=1000,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    predict_with_generate=True,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_strategy = 'epoch',\n",
    "    evaluation_strategy = 'epoch',\n",
    "    save_total_limit = 2,\n",
    "    logging_strategy = 'epoch',\n",
    "    logging_dir=\"./ckpt/t5/logs\",\n",
    "    load_best_model_at_end = True,\n",
    "    learning_rate=learning_rate,\n",
    "    remove_unused_columns = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, context):\n",
    "    inputs = f'question: {question}  context: {context} </s>'\n",
    "    sample = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True, return_tensors='pt')\n",
    "    sample = sample.to(\"cuda:0\")\n",
    "    outputs = model.generate(**sample, max_length=max_target_length, num_beams=num_beams)\n",
    "    pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    pred = \"\\n\".join(nltk.sent_tokenize(pred))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = ''''ㅋㅋ'은 일반적으로 웃음소리를 표현할 때 사용된다. 통신체에서는 웃는 모습을 직접적으로 보여줄 수 없기 때문에 의성어를 사용하는 경우가 많은데 그 중에서 '킥킥', '큭큭'을 초성체로 바꾸어 사용한 것이 'ㅋㅋ'에 해당한다. 이에 대해 2001년 연구에서는 \"자음만 가지고 표시하여 통신상의 재미를 더하기 위한 방법으로 사용되고 있다\"고 해석한 바 있다. \\n\\nㅋ자를 웃음소리를 의미하는 것으로 사용할 때, 'ㅋ'자 한 개에서 'ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ...'하는 식으로 열 타 이상 한 줄 넘게까지 쓰이기도 한다 ㅋ자를 얼마나 많이 썼느냐에 따라 그 느낌과 의미는 달라진다. 신조어를 전문으로 연구하는 이재현 문화평론가는 《한국일보》의 기고글에서 그 종류와 느낌이 대략 다음과 같다고 보았다\\n* ㅋ - 일반적으로 무심하게 동의하려는 상황에서 쓰인다. 다른 말끝에 붙였을 때, 이를테면 '그래ㅋ', '좋네ㅋ', '올ㅋ' 등은 웃음 자체라기보다는 입꼬리가 아주 살짝 올라간 상황이라는 느낌을 준다. 의성어로는 \"큭\" 혹은 \"킥\"으로 해석될 수 있다. 한편 '뭐해?ㅋ', '미안ㅋ'의 경우 'ㅋ'는 상투적인 군말의 역할을 한다.\\n* ㅋㅋ - 위의 것보다 ㅋ자가 하나가 더 많으나 오히려 형식적이라는 느낌을 준다. 좋게는 '그렇군'을 뜻하거나, 조금 나쁘게는 대화상에서 의례적으로 추임새를 넣고 있다는 느낌을 준다. 'ㅋㅋ'은 미혼남녀들이 가장 싫어하는 성의 없는 메신저 말투로 선정되기도 했다. 다만 회사 상사에게 대답하는 경우 같은 공적인 상황에서는 '네ㅋㅋ'와 같은 대답은 가벼워 보인다는 조사도 있었다. \\n* ㅋㅋㅋ - 비교적 중립적이면서도 가장 많이 사용되는 용법이다. 웃는 감정을 그대로 표현한다는 뉘앙스를 갖는다.\\n* ㅋㅋㅋㅋ... - 여기서부터는 \"정말 웃긴다\"라는 반응을 나름 정성들여서 표현하는 느낌을 준다. 네 개부터 그 이상은 의미의 함축과 정서의 강도가 거의 같으며, 이런 점에서 'ㅋㅋㅋㅋ'는 실질적으로 웃긴다는 것을 표현하는 최소 조건에 해당한다.'''\n",
    "question = '웃는 감정을 그대로 표현한다는 뉘앙스를 갖는 ㅋ의 용법은?'\n",
    "generated_text = generate_answer(question, context)\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = '''냉대 동계 소우 기후(冷带冬季少雨氣候)는 쾨펜의 기후 구분에서 냉대 기후에 속하며, 기호는 Dwa, Dwb, Dwc, Dwd로, D는 냉대, w는 동계건조(wintertrocken)을 가리킨다. 트란스 바이칼 기후 라고도 한다. 여름에는 온도가 비교적 높고, 겨울에는 맑은 날씨가 지속되고 약한 바람이 분다. 복사냉각 때문에 매우 쌀쌀한 것이 특징이다. 또 여름에는 계절풍의 영향으로 비가 많이 내린다. 겨울은 여름에 비해 매우 긴 편이다. 기온의 연교차가 매우 크게 나타나는 대륙성 기후가 짙게 나타난다.\\n\\n냉대 동계 소우 기후는 매우 추운 기후로, 농작물을 재배할 수 있는 곳과 재배 수종 및 수가 제한되어 있다. 수수, 옥수수, 봄밀 등을 재배한다. 북부는 타이가 기후와 같이 타이가가 많이 나타나 임업활동을 한다. 숲 또한 냉대 습윤 기후와 비슷하여 남부에는 침엽수와 활엽수가 함께 나타나는 혼합림이, 북부에는 타이가(침엽수림)이 잘 나타난다. 북부는 추위에도 잘 견디는 가늘고 뾰족한 잎을 가진 침엽수가 자라기 유리하기 때문에 타이가가, 남부에는 북부보다 따뜻하여 혼합림이 잘 나타난다. 남부 지역의 극소수에서만 벼 재배가 가능하다.\\n\\n토양은 주로 포드졸이 나타나고, 혼합림에서는 갈색 삼림토가 나타나는데, 포드졸은 유기물이 분해되지는 않았으나, 소금기를 잘 머금지 않고 침엽수림의 특성 때문에 농작물을 재배하는 데에는 적합하지 않다. 그에 반해 갈색 삼림토는 물을 잘 흡수하고 포드졸에 비해 약산성을 띠기 때문에 농작물 재배와 목축에 알맞다.\\n\\n이런 냉대 동계 소우 기후가 나타나는 곳으로는 시베리아 내륙 동부, 중국 화베이 동부 및 둥베이, 대한민국 강원도 내륙 지역과 경기도 및 산악지대, 조선민주주의인민공화국 대부분, 캐나다 및 미국, 네팔의 일부 지역 등이 있으며, 특히 시베리아 동부는 세계의 한극을 만들어 낸다.\\n\\n그래서 이 기후가 냉대 기후의 탈을 쓴 사바나 기후라고 봐야 하는 수준에 이르게 되며, 겨울철에는 강수량이 적어져서 그렇게 건조한 만큼 불씨가 쉽게 일어나는 특성상 산불도 물론 일어나기가 쉬운 기후 대역에 속한다.\\n\\n앞으로 온난화 여파에 따라 이 기후가 사라질 가능성이 있는 지역도 생길 수도 있다. 단적인 예로 대한민국의 수도권, 강원도 등은 현재 Dwa로 분류되나 온대 기후로 바뀔 가능성이 있다.'''\n",
    "question = '냉대 동계 소우 기후의 w는 무엇의 약자인가?'\n",
    "generated_text = generate_answer(question, context)\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = '''한국의 웹툰.\\n\\n곽백수가 네이버에서 연재한 웹툰. 회사 생활의 이야기를 주축으로 하는 직장 만화로, 작가의 이전 흥행작인《트라우마》에서도 익히 이름을 알렸던 가우스 그룹#s-1.3이 아예 메인으로 다뤄진다. 따라서, 어떤 의미에서는 트라우마의 스핀오프작이라고도 볼수 있는 셈. 물론, 가우스 그룹의 막나가는 센스는 여전하다. 초기에는 《사이좋은 가우스전자》라는 이름이었지만 어느 순간 '사이좋은'이 빠지고 그냥 《가우스전자》가 되었다.\\n\\n기본적으로《트라우마》와 비슷한 분위기의 개그물이지만 블랙 코미디의 성격이 강하다. 게다가 주제가 주제인지라 회사원 입장에서 공감할 내용들이 많으며, 그런 독자들의 반응은 대개 '내가 웃어도 웃는 게 아니야.' 로 통일되어 있다. 그래도 불편한 진실과 블랙 코미디 사이에서 블랙 코미디 쪽으로 줄타기를 잘 하고 있다. 또한 브리핑 에피소드들을 보면 현 업계의 시류를 잘 집고 있어 작가가 에피소드 소재에 대한 준비를 열심히 한 듯한 느낌이 난다. 그렇기에 특히 회사원들, 즉 어느 정도 연령층이 높은 독자들에게 평이 상당히 좋다. 그래도 상대적으로 악역 포지션인 인물들이 있고 그렇지 않은 인물들이 있지만, 현실 기업과는 비교도 할 수 없을만큼의 훈훈함이 특징.'''\n",
    "question = '《가우스전자》의 원래 제목은 무엇이었나?'\n",
    "generated_text = generate_answer(question, context)\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
